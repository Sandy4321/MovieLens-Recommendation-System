{"cells":[{"cell_type":"markdown","source":["## Movie recommender system with Spark machine learning\n### MovieLens\nMovieLens is a project developed by GroupLens, a research laboratory at the University of Minnesota. MovieLens provides an online movie recommender application that uses anonymously-collected data to improve recommender algorithms. Anyone can try the app for free and get movies recommendations. To help people develop the best recommendation algorithms, MovieLens also released several data sets. In this notebook, we'll use the latest data set, which has two sizes.\n\nThe full data set consists of more than 24 million ratings across more than 40,000 movies by more than 250,000 users. The file size is kept under 1GB by using indexes instead of full string names.\n\nThe small data set is a subset of the full data set. It's generally a good idea to start building a working program with a small data set to get faster performance while interacting, exploring, and getting errors with your data. When we have a fully working program, we can apply the same code to the larger data set, possibly on a larger cluster of processors. We can also minimize memory consumption by limiting the data volume as much as possible, for example, by using indexes.\n\n### Spark machine learning library\nThe library has two packages:\n* spark.mllib contains the original API that handles data in RDDs. It's in maintenance mode, but fully supported.\n* spark.ml contains a newer API for constructing ML pipelines. It handles data in DataFrames. It's being actively enhanced."],"metadata":{}},{"cell_type":"markdown","source":["### 1. Load the data\nWe'll create Spark DataFrames, which are similar to R or pandas DataFrames, but can be distributed on a cluster of Spark executors, which can potentially scale up to thousands of machines. DataFrames are one of the easiest and best performing ways of manipulating data with Spark, but they require structured data in formats or sources such as CSV, Parquet, JSON, or JDBC.\n\nHere we will use files movie.csv and ratings.csv."],"metadata":{}},{"cell_type":"code","source":["%fs ls /FileStore/tables/movies.csv"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["movies = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load('/FileStore/tables/movies.csv')\n\nmovies.cache()\ndisplay(movies)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["ratings = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load('/FileStore/tables/ratings.csv')\n\nratings.cache()\ndisplay(ratings)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### 2. Explore the data with Spark APIs"],"metadata":{}},{"cell_type":"code","source":["# Print out the schema of the two tables\nmovies.printSchema()\nratings.printSchema()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Run the describe( ) method to see the count, mean, standard deviation, minimum, and maximum values for the data in each column."],"metadata":{}},{"cell_type":"code","source":["display(ratings.describe())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# See how many distinct ratings, users and movies we have total. Also the number of good movies (with at least one rating >4).\nnumRatings=ratings.count()\nnumUsers=ratings.select(\"userId\").distinct().count()\nnumMovies=ratings.select(\"movieId\").distinct().count()\n\nprint \"Number of different ratings: \"+str(numRatings)\nprint \"Number of different users: \"+str(numUsers)\nprint \"Number of different movies: \"+str(numMovies)\n\nnumGoodMovies=ratings.filter(\"rating > 4\").select(\"movieId\").distinct().count()\nprint \"Number of movies with at least one rating strictly higher than 4: \"+str(numGoodMovies)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Counts the ratings for each movie\nmovies_counts=ratings.select(\"movieId\",\"rating\").groupBy(\"movieId\").count().orderBy(\"movieId\").toDF(\"movieId\",\"counts\")\ndisplay(movies_counts)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### 3. Visualize the data\n\nwe'll use the Seaborn and matplotlib matplotlib libraries to create graphs. The Seaborn library works with the matplotlib library to graph statistical data."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nratings_DF=ratings.toPandas()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Create the graph on a larger scale with the color palette:"],"metadata":{}},{"cell_type":"code","source":["sns.lmplot(x=\"userId\",y=\"movieId\",hue=\"rating\",data=ratings_DF,fit_reg=False,size=10,aspect=2,palette=sns.diverging_palette(10,133,sep=80,n=10))\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["On this matrix, you'll notice gaps in the data: some movies and users are missing. This is because you're using a subset of the data.\n\nNevertheless, you can identify some patterns. Some users always give positive reviews of movies. Some movies are rated a lot, which could be for different reasons, such as the first release of the MovieLens website had a much smaller catalog, or the movies are more famous."],"metadata":{}},{"cell_type":"markdown","source":["### 4. Build the recommender system\n\nThere are different methods for building a recommender system, such as, user-based, content-based, or collaborative filtering. Collaborative filtering calculates recommendations based on similarities between users and products. For example, collaborative filtering assumes that users who give the similar ratings on the same movies will also have similar opinions on movies that they haven't seen.\n\nThe alternating least squares (ALS) algorithm provides collaborative filtering between users and products to find products that the customers might like, based on their previous ratings.\n\nIn this case, the ALS algorithm will create a matrix of all users (row) versus all movies (col). Most cells in the matrix will be empty. An empty cell means the user hasn't reviewed the movie yet. The ALS algorithm will fill in the probable ratings, based on similarities between user ratings and similarities between movies. The algorithm uses the least squares computation to minimize the estimation errors, and alternates between solving for movie factors and solving for user factors.\n\nThe following trivial example gives you an idea of the problem to solve. However, keep in mind that the general problem is much harder because the matrix often has far more missing values."],"metadata":{}},{"cell_type":"code","source":["# create table so we can use SQL\nratings.createOrReplaceTempView(\"ratings_sql\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%sql  /* Check the size of the ratings matrix   */\nselect *, nb_ratings/matrix_size*100 as percentage\nfrom (\n  select *, nb_users*nb_movies as matrix_size\n  from (\n    select count(distinct(userId)) as nb_users, count(distinct(movieId)) as nb_movies, count(*) as nb_ratings\n    from ratings_sql\n  )\n  )"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Less than 2% of the matrix is filled."],"metadata":{}},{"cell_type":"markdown","source":["####4.1 Train the model\n\nUse the SparkML ALS algorithm to train a model to provide recommendations. The mandatory parameters to the ALS algorithm are the columns that identify the users, the items, and the ratings. Run the fit() method to train the model:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALS\nmodel= ALS(userCol=\"userId\",itemCol=\"movieId\",ratingCol=\"rating\").fit(ratings)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["#### 4.2 Run the model\n\nRun the transform( ) method to score the model and output a DataFrame with an additional prediction column that shows the predicted rating"],"metadata":{}},{"cell_type":"code","source":["predictions=model.transform(ratings)\ndisplay(predictions)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["You can see that many of the predictions are close to the actual ratings."],"metadata":{}},{"cell_type":"markdown","source":["#### 4.3 Evaluate the model\n\nAfter you apply a model to a data set, you should evaluate the performance of the model by comparing the predicted values with the original values. Use the RegressionEvaluator method to compare continuous values with the root mean squared calculation. The root mean squared error calculation measures the average of the squares of the errors between what is estimated and the existing data. The lower the mean squared error value, the more accurate the model."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"rating\",predictionCol=\"prediction\")\nprint \"the root mean squared error is:\"+str(evaluator.evaluate(predictions))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["You want the performance score to improve with your design iterations so that the model is improved. But notice that you just ran the training and the scoring on the same data set. That's something that you won't normally do because you usually want to predict values that you don't already know! Therefore, this result is nonsense. To accurately evaluate the model, it's common practice in machine learning to split the data set between a training data set to train the model, and a test data set to compare the predicted results with the original results. This process is called cross-validation. Not doing cross-validation often leads to overfitting, which occurs when the model is too specific to the training data set and does not perform well on a more general data set."],"metadata":{}},{"cell_type":"markdown","source":["#### 4.4 Split the data set\n\nSplit your ratings data set between an 80% training data set and a 20% test data set. Then rerun the steps to train, run, and evaluate the model."],"metadata":{}},{"cell_type":"code","source":["ratings_train, ratings_test=ratings.randomSplit([0.8, 0.2])\nmodel= ALS(userCol=\"userId\",itemCol=\"movieId\",ratingCol=\"rating\").fit(ratings_train)\npredictions=model.transform(ratings_test)\ndisplay(predictions)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"rating\",predictionCol=\"prediction\")\nprint \"the root mean squared error is:\"+str(evaluator.evaluate(predictions))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["You might get the value nan (not a number) from the previous cell."],"metadata":{}},{"cell_type":"markdown","source":["#### 4.5 Handle NaN results\nA NaN result is because the model can't predict values for users for which there's no data. A temporary workaround is to exclude rows with predicted NaN values or to replace them with a constant, for example, the general mean rating. However, to map to a real business problem, the data scientist, in collaboration with the business owner, must define what happens if such an event occurs. For example, you can provide no recommendation for a user until that user rates a few items. Alternatively, before user rates five items, you can use a user-based recommender system that's based on the user's profile (that's another recommender system to develop).\n\nReplace predicted NaN values with the average rating and evaluate the model:"],"metadata":{}},{"cell_type":"code","source":["ratings_avg=ratings.select('rating').groupBy().avg().first()[0]\nevaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"rating\",predictionCol=\"prediction\")\nprint \"the root mean squared error is:\"+str(evaluator.evaluate(predictions.na.fill(ratings_avg)))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Or exclude predicted NaN values and evaluate the model:"],"metadata":{}},{"cell_type":"code","source":["evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"rating\",predictionCol=\"prediction\")\nprint \"the root mean squared error is:\"+str(evaluator.evaluate(predictions.na.drop()))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Obviously, you get lower performance than with the previous model, but you're protected against overfitting: you will actually get this level of performance on new incoming data!"],"metadata":{}},{"cell_type":"markdown","source":["#### 4.6 Improve the performance score\nIf you run the randomSplit(), fit(), transform(), and evaluate() functions several times, you won't always get the same performance score. This is because the randomSplit() and ALS() functions have some randomness. To get a more precise performance score, run the model several times and then compute the average performance score. This process is really close to k-fold cross validation.\n\nCreate a repeatALS function that trains, runs, and evaluates the model multiple times:"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\ndef repeatALS (data, k, userCol=\"userId\", itemCol=\"movieId\",ratingCol=\"rating\",metricName=\"rmse\"):\n  evaluations=[]\n  for i in range(1,k+1):\n    train,test=data.randomSplit([k-1.0,1.0])\n    model= ALS(userCol=userCol,itemCol=itemCol,ratingCol=ratingCol).fit(train)\n    predictions=model.transform(test)\n    evaluator=RegressionEvaluator(metricName=metricName,labelCol=\"rating\",predictionCol=\"prediction\")\n    evaluation=evaluator.evaluate(predictions.na.drop())\n    print \"Loop\"+str(i)+\":\"+metricName+\"=\"+str(evaluation)\n    evaluations.append(evaluation)\n  return np.mean(evaluations)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["repeatALS(ratings,k=4)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["The computed performance score is more stable this way.\n\nCreate a kfoldALS function that also trains, runs, and evaluates the model, but splits up the data between training and testing data sets in a different way. The original data set is split into k data sets. Each of the k iterations of the function uses a different data set for testing and the other data sets for training."],"metadata":{}},{"cell_type":"code","source":["def kfoldALS (data, k, userCol=\"userId\", itemCol=\"movieId\",ratingCol=\"rating\",metricName=\"rmse\"):\n  evaluations=[]\n  weights=[1.0]*k\n  splits=data.randomSplit(weights)\n  for i in range(0,k):\n    test=splits[i]\n    train=spark.createDataFrame(sc.emptyRDD(),data.schema)\n    for j in range(0,k):\n      if i==j: continue\n      else: train=train.union(splits[j])\n    model= ALS(userCol=userCol,itemCol=itemCol,ratingCol=ratingCol).fit(train)\n    predictions=model.transform(test)\n    evaluator=RegressionEvaluator(metricName=metricName,labelCol=\"rating\",predictionCol=\"prediction\")\n    evaluation=evaluator.evaluate(predictions.na.drop())\n    print \"Loop\"+str(i)+\":\"+metricName+\"=\"+str(evaluation)\n    evaluations.append(evaluation)\n  return np.mean(evaluations)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["kfoldALS(ratings,k=4)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["kfoldALS(ratings,k=10)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["The bigger the training set is, the better performances you get. A general assumption in machine learning is that more data usually beats a better algorithm. You can easily improve this performance score by using the full data set."],"metadata":{}},{"cell_type":"markdown","source":["#### 4.7 Improve the model\nSo now, how can we improve this model? Machine learning algorithms have hyperparameters that control how the algorithm works.\n\nThe ALS algorithm has this signature. The ALS hyperparameters are:\n\n* rank = the number of latent factors in the model\n* maxIter = the maximum number of iterations\n* regParam = the regularization parameter\nTo test several values for those hyperparameters and choose the best configuration, it's common practice to define a grid of parameter combinations and to run a grid search over the combinations to evaluate the resulting models and comparing their performance. This process is known as model selection.\n\nThe Spark CrossValidator function performs a grid search as well as k-fold cross validation. Run the CrossValidator function with multiple values for rank and regParam:\n\nclass pyspark.ml.recommendation.ALS(\n        rank=10,\n        maxIter=10,\n        regParam=0.1,\n        numUserBlocks=10,\n        numItemBlocks=10,\n        implicitPrefs=false,\n        alpha=1.0,\n        userCol=\"user\",\n        itemCol=\"item\",\n        seed=None,\n        ratingCol=\"rating\",\n        nonnegative=false,\n        checkpointInterval=10,\n        intermediateStorageLevel=\"MEMORY_AND_DISK\",\n        finalStorageLevel=\"MEMORY_AND_DISK\"\n    )"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\nratings_train, ratings_validation = ratings.randomSplit([90.0, 10.0])\nals = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n\nparamGrid = ParamGridBuilder().addGrid(als.rank, [1,5,10]).addGrid(als.maxIter, [20]).addGrid(als.regParam, [0.05,0.1,0.5]).build()\n\ncrossval = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\ncvModel = crossval.fit(ratings_train)   # Run cross-validation, and choose the best set of parameters.\npredictions = cvModel.transform(ratings_validation)\n\n\n#print \"The best model is:\"+ str(cvModel.getEstimatorParamMaps())\nprint \"The root mean squared error is: \" + str(evaluator.evaluate(predictions.na.drop()))"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["The more folds and parameters you add to the grid, the longer it takes to test any combination. The CrossValidator model contains more information about the performance for each combination that you can get with the avgMetrics() method. For example, you can graph the results on a plot for analysis.\n\nUnfortunately, because of the SPARK-14489 issue mentioned above, the CrossValidator function can't compute the root mean squared error most of the time and provides incorrect results. You could limit this problem by making the training set much larger than the test set, but that's not a good practice. If you want to learn more about this issue, which is more a conceptual one than a technical one, and how this is being solved in the next Spark 2.2 release, you can have a look at Nick Pentreath's pull request #12896. Welcome to the Open Source world!"],"metadata":{}},{"cell_type":"markdown","source":["#### 4.8 Recommend movies"],"metadata":{}},{"cell_type":"code","source":["import math\ndef dcg (list, k):\n  dcg=0\n  for i in range(k):\n      dcg=dcg+list[i]/math.log(i+2,2)\n  return dcg"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["Use NDCG (Normalized Discounted Cumulative Gain) to evaluate the similarity of two ranking lists."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nUserId=predictions.select(\"userId\").distinct().rdd.flatMap(lambda x: x).collect()   # distinct users\nNDCG=[]\nfor i in range(len(UserId)):\n  ID=UserId[i]\n  subDF=predictions.filter(col(\"userId\")==ID).orderBy(\"prediction\", ascending=False)\n  l1 = subDF.select(\"rating\").na.drop().rdd.flatMap(lambda x: x).collect()\n  l2 = sorted(l1, reverse=True)\n  k=len(l1)\n  ndcg=dcg(l1,k)/dcg(l2,k)\n  print ndcg\n  NDCG.append(ndcg)\n\nnp.mean(NDCG)\nnp.max(NDCG)\nnp.min(NDCG)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["predictions.createOrReplaceTempView(\"predictions_sql\")\nmovies.createOrReplaceTempView(\"movies_sql\")"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["%sql\n/* See the recommend movies for user with ID=10 */\nselect p.userId, p.movieId, p.rating, p.prediction, m.title, m.genres\nfrom predictions_sql p\njoin movies_sql m on p.movieId=m.movieId\nwhere p.userId == 10\norder by p.prediction desc"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["### 5. Generalize the model to full data set."],"metadata":{}},{"cell_type":"code","source":["# Load the full data set\nmovies_full = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load('/FileStore/tables/movies_full.csv')\n\nratings_full = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load('/FileStore/tables/ratings_full.csv')\n\nratings.createOrReplaceTempView(\"ratings_full_sql\")"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["%sql  /* Check the size of the ratings matrix   */\nselect *, nb_ratings/matrix_size*100 as percentage\nfrom (\n  select *, nb_users*nb_movies as matrix_size\n  from (\n    select count(distinct(userId)) as nb_users, count(distinct(movieId)) as nb_movies, count(*) as nb_ratings\n    from ratings_full_sql\n  )\n  )\n/* Less than 2% of the matrix is filled. */"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["ratings_full_train, ratings_full_validation = ratings.randomSplit([90.0, 10.0])\nals = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n\nparamGrid = ParamGridBuilder().addGrid(als.rank, [1,5,10]).addGrid(als.maxIter, [20]).addGrid(als.regParam, [0.05,0.1,0.5]).build()\n\ncrossval = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\ncvModel_full = crossval.fit(ratings_full_train)   # Run cross-validation, and choose the best set of parameters.\npredictions_full = cvModel_full.transform(ratings_full_validation)\n\nprint \"The root mean squared error is: \" + str(evaluator.evaluate(predictions_full.na.drop()))"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["import math\ndef dcg (list, k):\n  dcg=0\n  for i in range(k):\n      dcg=dcg+list[i]/math.log(i+2,2)\n  return dcg\n\n\nUserId_full=predictions_full.select(\"userId\").distinct().rdd.flatMap(lambda x: x).collect()   # distinct users\nNDCG_full=[]\nfor i in range(len(UserId_full)):\n  ID_full=UserId_full[i]\n  subDF_full=predictions_full.filter(col(\"userId\")==ID_full).orderBy(\"prediction\", ascending=False)\n  l1_full = subDF_full.select(\"rating\").na.drop().rdd.flatMap(lambda x: x).collect()\n  l2_full = sorted(l1_full, reverse=True)\n  k=len(l1_full)\n  ndcg_full=dcg(l1_full,k)/dcg(l2_full,k)\n  print ndcg_full\n  NDCG_full.append(ndcg_full)\n\nnp.mean(NDCG_full)\nnp.max(NDCG_full)\nnp.min(NDCG_full)\n\n\npredictions_full.createOrReplaceTempView(\"predictions_full_sql\")\nmovies_full.createOrReplaceTempView(\"movies_full_sql\")"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["%sql\n/* See the recommend movies for user with ID=10 */\nselect p.userId, p.movieId, p.rating, p.prediction, m.title, m.genres\nfrom predictions_full_sql p\njoin movies_full_sql m on p.movieId=m.movieId\nwhere p.userId == 10\norder by p.prediction desc"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["Reference: https://datascience.ibm.com/exchange/public/entry/view/99b857815e69353c04d95daefb3b91fa"],"metadata":{}}],"metadata":{"name":"MovieLens Recommender","notebookId":421047859161014},"nbformat":4,"nbformat_minor":0}
